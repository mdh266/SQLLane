{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa3236b",
   "metadata": {},
   "source": [
    "# A Trip Down SQLane: 13 Tips and Tricks For SQL\n",
    "-------------------------------\n",
    "\n",
    "## I. Introduction\n",
    "------------------\n",
    "\n",
    "In this post I want to take a trip down SQL memory lane, or just SQLLane for short. [Structured Query Language (SQL)](https://en.wikipedia.org/wiki/SQL) is something that I have been using in many different forms for years. On this blog I have written prior posts about [SQLite and PostgreSQL](https://michael-harmon.com/posts/sqlwars/), [NoSQL](https://michael-harmon.com/posts/sentimentanalysis1/) and [DuckDB](https://michael-harmon.com/posts/polarsduckdb/). Elsewhere  I have used [Postgres](https://www.postgresql.org/), [Teradata](https://www.teradata.com/), [Snowflake](https://www.snowflake.com/en/), [Impala](https://impala.apache.org/), [HiveQL](https://hive.apache.org/) and [SparkSQL](https://spark.apache.org/sql/). SparkSQL and [Apache Spark](https://spark.apache.org/) more generally holds a special place in my heart. The ability to switch between SQL statements and dataframe operations as well as incorporate arbitrary transformation and actions using Python, Scala or Java make Spark an incredibly powerful tool. \n",
    "\n",
    "In this post, I will go over techniques that have been extremely helpful in the past. These wont be introductory techinques or queries; the internet is littered with those. I'll go over some more intermediate and lesser known querying techniques. The main topics I'll cover are:\n",
    "\n",
    "1. [Conditional Statements](https://www.geeksforgeeks.org/sql/sql-conditional-expressions/)\n",
    "2. [Window Functions](https://www.geeksforgeeks.org/sql/window-functions-in-sql/)\n",
    "3. [Array Operations](https://www.postgresql.org/docs/current/functions-array.html)\n",
    "4. [Special Types of Joins](https://www.w3schools.com/sql/sql_join.asp)\n",
    "\n",
    "I'll also make a few notes of specifics to Spark that are useful in practice. One thing to note is that I use SparkSQL for both SQL queries as well as dataframe operations. The Spark API is exteremely well written and the syntax mirrors SQL so closely I usually just think of the two as interchangable. To some degree they are, but I have found in a few specific cases using the dataframe API provides advantages that I will call out. The last thing I will say is that I will try to be a little more succinct in this post and realistic examples from online platforms such as [LeetCode](https://leetcode.com/) and [DataLemur](https://leetcode.com/) to give you an idea of how to apply these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02fa4fb",
   "metadata": {},
   "source": [
    "## II. Conditional Expressions\n",
    "-------------------------\n",
    "Conditional expressions are queries that involve actions which are dependent on certain conditions being met. These are called \"if-else\" statements in other languages. I'll start out with simple functions for text that require if-then staetements under-the-hood. \n",
    "\n",
    "### 1.  TRIM, LOWER, And Regular Expressions \n",
    "These functions are extremely helpful when it comes to text. The [TRIM](http://w3schools.com/sql/func_sqlserver_trim.asp) function removes extra white spaces around text. There are versions which only remove extra spaces on the left side [LTRIM](https://www.w3schools.com/sql/func_sqlserver_ltrim.asp) and right side [RTRIM](https://www.w3schools.com/sql/func_sqlserver_rtrim.asp). The [LOWER](https://www.w3schools.com/sql/func_sqlserver_lower.asp) function converts all text to lower case (or [UPPER](https://www.w3schools.com/sql/func_sqlserver_upper.asp) if you prefer upper case). Lastly, regular expressions are extremely helpful in SQL since they are optimized operations. One particularly helpful technique is [REGEX_REPLACE](https://duckdb.org/docs/stable/sql/functions/regular_expressions#regexp_replacestring-pattern-replacement-options) which searches for text that meets a pattern and replaces with specified text. Let's go through a simple example.\n",
    "\n",
    "Say I am searching for all records of \"Michael Harmon\" in the database shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2a59238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬──────────────────────┐\n",
       "│  ID   │         name         │\n",
       "│ int32 │       varchar        │\n",
       "├───────┼──────────────────────┤\n",
       "│     1 │ Michael Harmon       │\n",
       "│     2 │ Dr. Michael Harmon   │\n",
       "│     3 │ mr. michael harmon   │\n",
       "│     4 │  Michael Harmon      │\n",
       "│     5 │ David Michael Harmon │\n",
       "└───────┴──────────────────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "query1 = open('queries/create_names.sql', 'r').read()\n",
    "duckdb.query(query1)\n",
    "duckdb.query(\"SELECT id, name FROM names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144ff861",
   "metadata": {},
   "source": [
    "I should return expect to get records 1-4. If I write a simple naive query using `name = \"Michael Harmon' I would only get the first result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34d323bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬────────────────┐\n",
       "│  ID   │      name      │\n",
       "│ int32 │    varchar     │\n",
       "├───────┼────────────────┤\n",
       "│     1 │ Michael Harmon │\n",
       "└───────┴────────────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"SELECT id, name FROM names WHERE name = 'Michael Harmon'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ed3f6",
   "metadata": {},
   "source": [
    "Instead I'll use `TRIM(LOWER(name))` to make everything the same case and remove extra-spaces to capture record 4. Now I could use a wildcard for records 2 and 3,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80f7400a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬──────────────────────┐\n",
       "│  ID   │         name         │\n",
       "│ int32 │       varchar        │\n",
       "├───────┼──────────────────────┤\n",
       "│     1 │ Michael Harmon       │\n",
       "│     2 │ Dr. Michael Harmon   │\n",
       "│     3 │ mr. michael harmon   │\n",
       "│     4 │  Michael Harmon      │\n",
       "│     5 │ David Michael Harmon │\n",
       "└───────┴──────────────────────┘"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"SELECT id, name FROM names WHERE TRIM(LOWER(name)) LIKE '%michael harmon'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911e124a",
   "metadata": {},
   "source": [
    "But that would be a mistake since it would capture record 5! Instead let's use regular expression to remove Dr. and mr. and replace that text with blanks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30066f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬────────────────────┐\n",
       "│  ID   │        name        │\n",
       "│ int32 │      varchar       │\n",
       "├───────┼────────────────────┤\n",
       "│     1 │ Michael Harmon     │\n",
       "│     2 │ Dr. Michael Harmon │\n",
       "│     3 │ mr. michael harmon │\n",
       "│     4 │  Michael Harmon    │\n",
       "└───────┴────────────────────┘"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\"\n",
    "SELECT \n",
    "    id,\n",
    "    name\n",
    "FROM \n",
    "    names\n",
    "WHERE\n",
    "    TRIM(\n",
    "        REGEXP_REPLACE(\n",
    "                LOWER(name),'(mr.|dr.)', ''\n",
    "             )\n",
    "        ) = 'michael harmon'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad191cb",
   "metadata": {},
   "source": [
    "Now we can move on to truely conditional statements!\n",
    "\n",
    "### 2.  Conditional Statements\n",
    "In modern programming languages \"if-else\" statements are pretty common statements. In SQL the equivalent is \"CASE WHEN ... THEN ... ELSE ..\". You can enumerate any number of cases and the ELSE statement covers the case that dont match any of the ones specified.\n",
    "\n",
    "As a simple example let's say we want to know the number names that are less than 16 characters in the `names` table, we could add a new column to the table with this information as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c402696f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬──────────────────────┬────────────────────┐\n",
       "│  ID   │         name         │ less_than_15_chars │\n",
       "│ int32 │       varchar        │      boolean       │\n",
       "├───────┼──────────────────────┼────────────────────┤\n",
       "│     1 │ Michael Harmon       │ true               │\n",
       "│     2 │ Dr. Michael Harmon   │ false              │\n",
       "│     3 │ mr. michael harmon   │ false              │\n",
       "│     4 │  Michael Harmon      │ true               │\n",
       "│     5 │ David Michael Harmon │ false              │\n",
       "└───────┴──────────────────────┴────────────────────┘"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\" \n",
    "SELECT \n",
    "    id, \n",
    "    name, \n",
    "    CASE WHEN LENGTH(name) < 16 THEN TRUE\n",
    "         ELSE FALSE \n",
    "    END AS less_than_15_chars\n",
    "FROM \n",
    "    names\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf2c2be",
   "metadata": {},
   "source": [
    "You can add more conditions by adding more `CASE WHEN .. THEN ...` statments, but the line must always end with `END` statement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3799d1ae",
   "metadata": {},
   "source": [
    "### 3.  Conditional Statements With Aggregations\n",
    "\n",
    "Conditional satments can also be used in conjunction with aggregation functions to create more complex queries. For example, you might want to count the number of names that are shorter than a certain length such as shown below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01ffa8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────────────────┐\n",
       "│ count_less_than_15_chars │\n",
       "│          int128          │\n",
       "├──────────────────────────┤\n",
       "│                        2 │\n",
       "└──────────────────────────┘"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\" \n",
    "SELECT \n",
    "    SUM(CASE WHEN LENGTH(name) < 16 THEN 1 END) AS count_less_than_15_chars\n",
    "FROM \n",
    "    names\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f5e84",
   "metadata": {},
   "source": [
    "An exmaple of where I have used conditional statments with aaggregations is [Monthly Transcations I](https://leetcode.com/problems/monthly-transactions-i/description/) problem on Leetcode; the [solution](https://github.com/mdh266/SQL-Practice/blob/master/leetcode/monthly-transactions-i.sql) is on my GitHub page. :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d9a425",
   "metadata": {},
   "source": [
    "## III. Window Functions\n",
    "----------------------\n",
    "[Window functions](https://en.wikipedia.org/wiki/Window_function_(SQL)) are another extremely important concept in SQL. A window is a function which uses values from one or multiple rows that are related to one another through a so-called partition to return a value for each row. This is a little abstract, so an example would a company table that has an employee, their department and their salary.  Like below,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7ea8cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────────┬───────────────┬────────────┬────────┐\n",
       "│ employee_id │ employee_name │ department │ salary │\n",
       "│    int32    │    varchar    │  varchar   │ int32  │\n",
       "├─────────────┼───────────────┼────────────┼────────┤\n",
       "│           1 │ Alice Johnson │ Sales      │  90000 │\n",
       "│           2 │ Bob Smith     │ Marketing  │  75000 │\n",
       "│           3 │ Charlie Brown │ Sales      │  95000 │\n",
       "│           4 │ Diana Prince  │ Sales      │  70000 │\n",
       "│           5 │ Ethan Hunt    │ Marketing  │  20000 │\n",
       "└─────────────┴───────────────┴────────────┴────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "duckdb.query(open('queries/create_employees.sql', 'r').read())\n",
    "duckdb.query(\"SELECT employee_id, employee_name, department, salary FROM employees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc9c656",
   "metadata": {},
   "source": [
    "We could find the average salary per deparment with the aggregation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26c9931e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌────────────┬─────────────────┐\n",
       "│ department │ dept_avg_salary │\n",
       "│  varchar   │     double      │\n",
       "├────────────┼─────────────────┤\n",
       "│ Marketing  │         47500.0 │\n",
       "│ Sales      │         85000.0 │\n",
       "└────────────┴─────────────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\"\n",
    "SELECT \n",
    "    department,\n",
    "    AVG(salary) dept_avg_salary\n",
    "FROM\n",
    "    employees\n",
    "GROUP BY 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebcf91c",
   "metadata": {},
   "source": [
    "But what if we want to assign the employee with their department average? We could do a nested query where we perform the aggregation and then join on the department, but this is kind of sloppy. Instead we can partition employees by their department and aveage over deparment as shown, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c27e0bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────────┬───────────────┬────────────┬────────┬─────────────────┐\n",
       "│ employee_id │ employee_name │ department │ salary │ dept_avg_salary │\n",
       "│    int32    │    varchar    │  varchar   │ int32  │     double      │\n",
       "├─────────────┼───────────────┼────────────┼────────┼─────────────────┤\n",
       "│           1 │ Alice Johnson │ Sales      │  90000 │         85000.0 │\n",
       "│           3 │ Charlie Brown │ Sales      │  95000 │         85000.0 │\n",
       "│           4 │ Diana Prince  │ Sales      │  70000 │         85000.0 │\n",
       "│           2 │ Bob Smith     │ Marketing  │  75000 │         47500.0 │\n",
       "│           5 │ Ethan Hunt    │ Marketing  │  20000 │         47500.0 │\n",
       "└─────────────┴───────────────┴────────────┴────────┴─────────────────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\"\n",
    "SELECT \n",
    "    employee_id,\n",
    "    employee_name,\n",
    "    department,\n",
    "    salary,\n",
    "    AVG(salary) OVER(PARTITION BY department) AS dept_avg_salary\n",
    "FROM\n",
    "    employees\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d6bdee",
   "metadata": {},
   "source": [
    "Interestingly DuckDB returns the results in an order that reflects the partitioning instead of the original ordering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61c1855",
   "metadata": {},
   "outputs": [],
   "source": [
    "An example, would be to add a new column which is the  but you could think of it as rank each employees in a department by their salary. This woudl be translated to SQL as\n",
    "\n",
    "SELECT \n",
    "    employee,\n",
    "    department,\n",
    "    salary,\n",
    "    AVG(salary) OVER(PARTITION BY department) AS dept_avg_salary\n",
    "FROM\n",
    "    employees\n",
    "\n",
    "The `PARTITION BY` statement defines which \n",
    "\n",
    "### 4. RANK, DENSE_RANK, ROW_NUMBER \n",
    "* Rank, \n",
    "* DENSE_RANK: https://github.com/mdh266/SQL-Practice/blob/master/datalemur/sql-top-three-salaries.sql\n",
    "* ROW_NUMBER: https://github.com/mdh266/SQL-Practice/blob/master/datalemur/sql-third-transaction.sql\n",
    "* Why you need partition by key for large tables\n",
    "\n",
    "### 5. QUALIFY\n",
    "\n",
    "### 6. LEAD & LAG\n",
    "* Year over year growth\n",
    "https://github.com/mdh266/SQL-Practice/blob/master/datalemur/yoy-growth-rate.sql\n",
    "\n",
    "### 7.  Moving Averages\n",
    "* https://github.com/mdh266/SQL-Practice/blob/master/datalemur/rolling-average-tweets.sql\n",
    "\n",
    "### 8. Multiple Expressions in SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2846035",
   "metadata": {},
   "source": [
    "## IV. Array Operations\n",
    "-------------------\n",
    "\n",
    "### 9. COLLECT_SET/ARRAY_AGG\n",
    "\n",
    "** https://github.com/mdh266/SQL-Practice/blob/master/datalemur/frequently-purchased-pairs.sql\n",
    "\n",
    "### 10. EXPLODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feb6253",
   "metadata": {},
   "source": [
    "## IV. Special Joins\n",
    "----------------\n",
    "### 11. Cross Joins\n",
    "* https://github.com/mdh266/SQL-Practice/blob/master/datalemur/repeated-payments.sql\n",
    "\n",
    "### 11. Filtering by Join\n",
    "* Repartitioning after with SparkSQL\n",
    "\n",
    "### 12. Conditional Assignments With A Join\n",
    "\n",
    "### 13. Broadcast Join in SparkSQL\n",
    "\n",
    "### 14. Left Anti Join with SparkSQL\n",
    "* https://github.com/mdh266/SQL-Practice/blob/master/leetcode/CustomersDontOrder.sql\n",
    "* With a left join and filter (needs to be outside the nested join)!\n",
    "\n",
    "<!-- 14. Conditional Joins -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61332909",
   "metadata": {},
   "source": [
    "## V. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fbab43",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
