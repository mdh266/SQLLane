{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa3236b",
   "metadata": {},
   "source": [
    "# A Trip Down SQLane: Tips For SQL and Spark\n",
    "-------------------------------------\n",
    "\n",
    "## I. Introduction\n",
    "------------------\n",
    "\n",
    "In this post I want to take a trip down SQL memory lane, or just SQLLane for short. [Structured Query Language (SQL)](https://en.wikipedia.org/wiki/SQL) is something that I have been using in many different forms for years. On this blog I have written prior posts about [SQLite and PostgreSQL](https://michael-harmon.com/posts/sqlwars/), [NoSQL](https://michael-harmon.com/posts/sentimentanalysis1/) and [DuckDB](https://michael-harmon.com/posts/polarsduckdb/). Elsewhere  I have used [Postgres](https://www.postgresql.org/), [Teradata](https://www.teradata.com/), [Snowflake](https://www.snowflake.com/en/), [Impala](https://impala.apache.org/), [HiveQL](https://hive.apache.org/) and [SparkSQL](https://spark.apache.org/sql/). SparkSQL and [Apache Spark](https://spark.apache.org/) more generally holds a special place in my heart. The ability to switch between SQL statements and dataframe operations as well as incorporate arbitrary transformation and actions using Python, Scala or Java make Spark an incredibly powerful tool. \n",
    "\n",
    "In this post, I will go over techniques that have been extremely helpful in the past. These wont be introductory techinques or queries; the internet is littered with those. I'll go over some more intermediate and lesser known querying techniques. The main topics I'll cover are:\n",
    "\n",
    "1. [Conditional Statements](https://www.geeksforgeeks.org/sql/sql-conditional-expressions/)\n",
    "2. [Window Functions](https://www.geeksforgeeks.org/sql/window-functions-in-sql/)\n",
    "3. [Array Operations](https://www.postgresql.org/docs/current/functions-array.html)\n",
    "4. [Special Types of Joins](https://www.w3schools.com/sql/sql_join.asp)\n",
    "\n",
    "I'll also make a few notes of specifics to Spark that are useful in practice. One thing to note is that I use SparkSQL for both SQL queries as well as dataframe operations. The Spark API is exteremely well written and the syntax mirrors SQL so closely I usually just think of the two as interchangable. To some degree they are, but I have found in a few specific cases using the dataframe API provides advantages that I will call out. The last thing I will say is that I will try to be a little more succinct in this post and while I'll mostly be using fake data.\n",
    "\n",
    "First I'll create some fake data and then we can get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "213d3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "duckdb.query(open('queries/create_names.sql', 'r').read())\n",
    "duckdb.query(open('queries/create_employees.sql', 'r').read())\n",
    "duckdb.query(open('queries/create_timeseries.sql', 'r').read())\n",
    "duckdb.query(open('queries/create_homesales.sql', 'r').read())\n",
    "duckdb.query(open('queries/create_regions.sql', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02fa4fb",
   "metadata": {},
   "source": [
    "## II. Conditional Expressions\n",
    "-------------------------\n",
    "Conditional expressions are queries that involve actions which are dependent on certain conditions being met. These are called \"if-else\" statements in other languages. I'll start out with simple functions for text that require if-then staetements under-the-hood. \n",
    "\n",
    "### 1.  TRIM, LOWER, & Regular Expressions \n",
    "These functions are extremely helpful when it comes to text. The [TRIM](http://w3schools.com/sql/func_sqlserver_trim.asp) function removes extra white spaces around text. There are versions which only remove extra spaces on the left side [LTRIM](https://www.w3schools.com/sql/func_sqlserver_ltrim.asp) and right side [RTRIM](https://www.w3schools.com/sql/func_sqlserver_rtrim.asp). The [LOWER](https://www.w3schools.com/sql/func_sqlserver_lower.asp) function converts all text to lower case (or [UPPER](https://www.w3schools.com/sql/func_sqlserver_upper.asp) if you prefer upper case). Lastly, regular expressions are extremely helpful in SQL since they are optimized operations. One particularly helpful technique is [REGEX_REPLACE](https://duckdb.org/docs/stable/sql/functions/regular_expressions#regexp_replacestring-pattern-replacement-options) which searches for text that meets a pattern and replaces with specified text. Let's go through a simple example.\n",
    "\n",
    "Say I am searching for all records of \"Michael Harmon\" in the database shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a59238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬──────────────────────┐\n",
       "│  ID   │         name         │\n",
       "│ int32 │       varchar        │\n",
       "├───────┼──────────────────────┤\n",
       "│     1 │ Michael Harmon       │\n",
       "│     2 │ Dr. Michael Harmon   │\n",
       "│     3 │ mr. michael harmon   │\n",
       "│     4 │  Michael Harmon      │\n",
       "│     5 │ David Michael Harmon │\n",
       "└───────┴──────────────────────┘"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"SELECT id, name FROM names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144ff861",
   "metadata": {},
   "source": [
    "I should return expect to get records 1-4. If I write a simple naive query using `name = \"Michael Harmon' I would only get the first result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34d323bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬────────────────┐\n",
       "│  ID   │      name      │\n",
       "│ int32 │    varchar     │\n",
       "├───────┼────────────────┤\n",
       "│     1 │ Michael Harmon │\n",
       "└───────┴────────────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"SELECT id, name FROM names WHERE name = 'Michael Harmon'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ed3f6",
   "metadata": {},
   "source": [
    "Instead I'll use `TRIM(LOWER(name))` to make everything the same case and remove extra-spaces to capture record 4. Now I could use a wildcard for records 2 and 3,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80f7400a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬──────────────────────┐\n",
       "│  ID   │         name         │\n",
       "│ int32 │       varchar        │\n",
       "├───────┼──────────────────────┤\n",
       "│     1 │ Michael Harmon       │\n",
       "│     2 │ Dr. Michael Harmon   │\n",
       "│     3 │ mr. michael harmon   │\n",
       "│     4 │  Michael Harmon      │\n",
       "│     5 │ David Michael Harmon │\n",
       "└───────┴──────────────────────┘"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"SELECT id, name FROM names WHERE TRIM(LOWER(name)) LIKE '%michael harmon'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911e124a",
   "metadata": {},
   "source": [
    "But that would be a mistake since it would capture record 5! Instead let's use regular expression to remove Dr. and mr. and replace that text with blanks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30066f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬────────────────────┐\n",
       "│  ID   │        name        │\n",
       "│ int32 │      varchar       │\n",
       "├───────┼────────────────────┤\n",
       "│     1 │ Michael Harmon     │\n",
       "│     2 │ Dr. Michael Harmon │\n",
       "│     3 │ mr. michael harmon │\n",
       "│     4 │  Michael Harmon    │\n",
       "└───────┴────────────────────┘"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\"\n",
    "SELECT \n",
    "    id,\n",
    "    name\n",
    "FROM \n",
    "    names\n",
    "WHERE\n",
    "    TRIM(\n",
    "        REGEXP_REPLACE(\n",
    "                LOWER(name),'(mr.|dr.)', ''\n",
    "             )\n",
    "        ) = 'michael harmon'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad191cb",
   "metadata": {},
   "source": [
    "Now we can move on to truely conditional statements!\n",
    "\n",
    "### 2.  Conditional Statements\n",
    "In modern programming languages \"if-else\" statements are pretty common statements. In SQL the equivalent is \"CASE WHEN ... THEN ... ELSE ..\". You can enumerate any number of cases and the ELSE statement covers the case that dont match any of the ones specified.\n",
    "\n",
    "As a simple example let's say we want to know the number names that are less than 16 characters in the `names` table, we could add a new column to the table with this information as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c402696f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬──────────────────────┬────────────────────┐\n",
       "│  ID   │         name         │ less_than_15_chars │\n",
       "│ int32 │       varchar        │      boolean       │\n",
       "├───────┼──────────────────────┼────────────────────┤\n",
       "│     1 │ Michael Harmon       │ true               │\n",
       "│     2 │ Dr. Michael Harmon   │ false              │\n",
       "│     3 │ mr. michael harmon   │ false              │\n",
       "│     4 │  Michael Harmon      │ true               │\n",
       "│     5 │ David Michael Harmon │ false              │\n",
       "└───────┴──────────────────────┴────────────────────┘"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\" \n",
    "SELECT \n",
    "    id, \n",
    "    name, \n",
    "    CASE WHEN LENGTH(name) < 16 THEN TRUE\n",
    "         ELSE FALSE \n",
    "    END AS less_than_15_chars\n",
    "FROM \n",
    "    names\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf2c2be",
   "metadata": {},
   "source": [
    "You can add more conditions by adding more `CASE WHEN .. THEN ...` statments without ever needing an `ELSE` (depending on the SQL dialect) but the stament must always end with `END` clause."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3799d1ae",
   "metadata": {},
   "source": [
    "### 3.  Conditional Statements With Aggregations\n",
    "\n",
    "Conditional satments can also be used in conjunction with aggregation functions to create more complex queries. For example, you might want to count the number of names that are shorter than a certain length such as shown below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01ffa8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────────────────┐\n",
       "│ count_less_than_15_chars │\n",
       "│          int128          │\n",
       "├──────────────────────────┤\n",
       "│                        2 │\n",
       "└──────────────────────────┘"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\" \n",
    "SELECT \n",
    "    SUM(CASE WHEN LENGTH(name) < 16 THEN 1 END) AS count_less_than_15_chars\n",
    "FROM \n",
    "    names\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f5e84",
   "metadata": {},
   "source": [
    "An exmaple of where I have used conditional statments with aaggregations is [Monthly Transcations I](https://leetcode.com/problems/monthly-transactions-i/description/) problem on Leetcode; the [solution](https://github.com/mdh266/SQL-Practice/blob/master/leetcode/monthly-transactions-i.sql) is on my GitHub page. :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d9a425",
   "metadata": {},
   "source": [
    "## III. Window Functions\n",
    "----------------------\n",
    "[Window functions](https://en.wikipedia.org/wiki/Window_function_(SQL)) are another extremely important concept in SQL. A window is a function which uses values from one or multiple rows that are related to one another through a so-called partition to return a value for each row. This is a little abstract, so an example would a company table that has an employee, their department and their salary.  Like below,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7ea8cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────────┬────────────────┬────────────┬────────┐\n",
       "│ employee_id │ employee_name  │ department │ salary │\n",
       "│    int32    │    varchar     │  varchar   │ int32  │\n",
       "├─────────────┼────────────────┼────────────┼────────┤\n",
       "│           1 │ Alice Johnson  │ Sales      │ 550000 │\n",
       "│           2 │ Bob Smith      │ Sales      │ 700000 │\n",
       "│           3 │ Charlie Brown  │ Sales      │ 320000 │\n",
       "│           4 │ Diana Prince   │ Sales      │ 620000 │\n",
       "│           5 │ Ethan Hunt     │ Sales      │ 410000 │\n",
       "│           6 │ Frank Green    │ Sales      │ 490000 │\n",
       "│           7 │ Grace Adams    │ Sales      │ 520000 │\n",
       "│           8 │ Henry King     │ Sales      │ 400000 │\n",
       "│           9 │ Ivy Walker     │ Sales      │ 540000 │\n",
       "│          10 │ Jack White     │ Sales      │ 470000 │\n",
       "│          11 │ Laura Chen     │ Sales      │ 380000 │\n",
       "│          12 │ Marcus Lee     │ Sales      │ 420000 │\n",
       "│          13 │ Nina Patel     │ Sales      │ 450000 │\n",
       "│          14 │ Oliver Stone   │ Operations │  85000 │\n",
       "│          15 │ Patricia Wells │ Operations │  78000 │\n",
       "│          16 │ Samuel Turner  │ Operations │  90000 │\n",
       "│          17 │ Tara Benson    │ Marketing  │  95000 │\n",
       "│          18 │ Uma Garcia     │ Marketing  │  88000 │\n",
       "│          19 │ Victor Ramirez │ Marketing  │  92000 │\n",
       "├─────────────┴────────────────┴────────────┴────────┤\n",
       "│ 19 rows                                  4 columns │\n",
       "└────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"SELECT employee_id, employee_name, department, salary FROM employees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc9c656",
   "metadata": {},
   "source": [
    "We could find the average salary per deparment with the aggregation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "26c9931e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌────────────┬───────────────────┐\n",
       "│ department │  dept_avg_salary  │\n",
       "│  varchar   │      double       │\n",
       "├────────────┼───────────────────┤\n",
       "│ Sales      │ 482307.6923076923 │\n",
       "│ Operations │ 84333.33333333333 │\n",
       "│ Marketing  │ 91666.66666666667 │\n",
       "└────────────┴───────────────────┘"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\"\n",
    "SELECT \n",
    "    department,\n",
    "    AVG(salary) dept_avg_salary\n",
    "FROM\n",
    "    employees\n",
    "GROUP BY 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebcf91c",
   "metadata": {},
   "source": [
    "But what if we want to assign the employee with their department average? We could do a nested query where we perform the aggregation and then join on the department, but this is kind of sloppy. Instead we can partition employees by their department and aveage over deparment as shown, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c27e0bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────────┬────────────────┬────────────┬────────┬───────────────────┐\n",
       "│ employee_id │ employee_name  │ department │ salary │  dept_avg_salary  │\n",
       "│    int32    │    varchar     │  varchar   │ int32  │      double       │\n",
       "├─────────────┼────────────────┼────────────┼────────┼───────────────────┤\n",
       "│          14 │ Oliver Stone   │ Operations │  85000 │ 84333.33333333333 │\n",
       "│          15 │ Patricia Wells │ Operations │  78000 │ 84333.33333333333 │\n",
       "│          16 │ Samuel Turner  │ Operations │  90000 │ 84333.33333333333 │\n",
       "│           1 │ Alice Johnson  │ Sales      │ 550000 │ 482307.6923076923 │\n",
       "│           2 │ Bob Smith      │ Sales      │ 700000 │ 482307.6923076923 │\n",
       "│           3 │ Charlie Brown  │ Sales      │ 320000 │ 482307.6923076923 │\n",
       "│           4 │ Diana Prince   │ Sales      │ 620000 │ 482307.6923076923 │\n",
       "│           5 │ Ethan Hunt     │ Sales      │ 410000 │ 482307.6923076923 │\n",
       "│           6 │ Frank Green    │ Sales      │ 490000 │ 482307.6923076923 │\n",
       "│           7 │ Grace Adams    │ Sales      │ 520000 │ 482307.6923076923 │\n",
       "│           8 │ Henry King     │ Sales      │ 400000 │ 482307.6923076923 │\n",
       "│           9 │ Ivy Walker     │ Sales      │ 540000 │ 482307.6923076923 │\n",
       "│          10 │ Jack White     │ Sales      │ 470000 │ 482307.6923076923 │\n",
       "│          11 │ Laura Chen     │ Sales      │ 380000 │ 482307.6923076923 │\n",
       "│          12 │ Marcus Lee     │ Sales      │ 420000 │ 482307.6923076923 │\n",
       "│          13 │ Nina Patel     │ Sales      │ 450000 │ 482307.6923076923 │\n",
       "│          17 │ Tara Benson    │ Marketing  │  95000 │ 91666.66666666667 │\n",
       "│          18 │ Uma Garcia     │ Marketing  │  88000 │ 91666.66666666667 │\n",
       "│          19 │ Victor Ramirez │ Marketing  │  92000 │ 91666.66666666667 │\n",
       "├─────────────┴────────────────┴────────────┴────────┴───────────────────┤\n",
       "│ 19 rows                                                      5 columns │\n",
       "└────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\"\n",
    "SELECT \n",
    "    employee_id,\n",
    "    employee_name,\n",
    "    department,\n",
    "    salary,\n",
    "    AVG(salary) OVER(PARTITION BY department) AS dept_avg_salary\n",
    "FROM\n",
    "    employees\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d6bdee",
   "metadata": {},
   "source": [
    "The `PARTITION BY` statement defines which group a row belongs to before perfoming an aggregation (in this case average) over the group. It is important to note that window functions do not reduce the number of rows returned. They just add additional columns based on calculations over a set of rows.\n",
    "\n",
    "\n",
    "Interestingly [DuckDB](https://duckdb.org) returns the results in an order that reflects the partitioning instead of the original ordering!\n",
    "\n",
    "### 4. RANK, DENSE_RANK & ROW_NUMBER \n",
    "One of the most common usages for window function is for ranking within a group. For example, say we want to rank each employee within a deparement based on their salary. We can do this in SQL as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8690e949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────────┬────────────────┬────────────┬────────┬──────────────────┐\n",
       "│ employee_id │ employee_name  │ department │ salary │ dept_salary_rank │\n",
       "│    int32    │    varchar     │  varchar   │ int32  │      int64       │\n",
       "├─────────────┼────────────────┼────────────┼────────┼──────────────────┤\n",
       "│          17 │ Tara Benson    │ Marketing  │  95000 │                1 │\n",
       "│          19 │ Victor Ramirez │ Marketing  │  92000 │                2 │\n",
       "│          18 │ Uma Garcia     │ Marketing  │  88000 │                3 │\n",
       "│          16 │ Samuel Turner  │ Operations │  90000 │                1 │\n",
       "│          14 │ Oliver Stone   │ Operations │  85000 │                2 │\n",
       "│          15 │ Patricia Wells │ Operations │  78000 │                3 │\n",
       "│           2 │ Bob Smith      │ Sales      │ 700000 │                1 │\n",
       "│           4 │ Diana Prince   │ Sales      │ 620000 │                2 │\n",
       "│           1 │ Alice Johnson  │ Sales      │ 550000 │                3 │\n",
       "│           9 │ Ivy Walker     │ Sales      │ 540000 │                4 │\n",
       "│           7 │ Grace Adams    │ Sales      │ 520000 │                5 │\n",
       "│           6 │ Frank Green    │ Sales      │ 490000 │                6 │\n",
       "│          10 │ Jack White     │ Sales      │ 470000 │                7 │\n",
       "│          13 │ Nina Patel     │ Sales      │ 450000 │                8 │\n",
       "│          12 │ Marcus Lee     │ Sales      │ 420000 │                9 │\n",
       "│           5 │ Ethan Hunt     │ Sales      │ 410000 │               10 │\n",
       "│           8 │ Henry King     │ Sales      │ 400000 │               11 │\n",
       "│          11 │ Laura Chen     │ Sales      │ 380000 │               12 │\n",
       "│           3 │ Charlie Brown  │ Sales      │ 320000 │               13 │\n",
       "├─────────────┴────────────────┴────────────┴────────┴──────────────────┤\n",
       "│ 19 rows                                                     5 columns │\n",
       "└───────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\"\n",
    "SELECT \n",
    "    employee_id, \n",
    "    employee_name,\n",
    "    department,\n",
    "    salary,\n",
    "    RANK() OVER(PARTITION BY department ORDER BY salary DESC) AS dept_salary_rank\n",
    "FROM\n",
    "    employees\n",
    "ORDER BY department, dept_salary_rank ASC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35acd094",
   "metadata": {},
   "source": [
    "This is actually pretty useful!  You can use [RANK](https://www.geeksforgeeks.org/sql/rank-function-in-sql-server/), [DENSE_RANK](https://www.geeksforgeeks.org/sql-server/rank-and-dense-rank-in-sql-server/), and [ROW_NUMBER](https://www.geeksforgeeks.org/sql-server/sql-server-row_number-function-with-partition-by/) to assign rankings within partitions of your data. The difference between them is how they handle ties. RANK will skip ranks if there are ties, DENSE_RANK won't skip any ranks, and ROW_NUMBER will assign a unique sequential number to each row, regardless of ties.\n",
    "\n",
    "One thing to be aware of is in Apache Spark if you apply ranking to dataframe without defining a paritioning such as below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273a0dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DuckDB with Spark\").getOrCreate()\n",
    "\n",
    "employee_df = spark.createDataFrame(duckdb.query(\"SELECT employee_id, employee_name, department, salary FROM employees\").to_df())\n",
    "win = Window.orderBy(F.col(\"salary\"))\n",
    "employee_df.withColumn(\"ranking\", F.row_number().over(win)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad4833",
   "metadata": {},
   "source": [
    "You will actually force the entire dataframe to collected to the driver to be sorted. If your dataframe VERY big this will cause you to blow out of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61c1855",
   "metadata": {},
   "source": [
    "### 5. QUALIFY\n",
    "\n",
    "[Qualify](https://www.datacamp.com/tutorial/qualify-the-sql-filtering-statement-you-never-knew-you-needed) is a function to filter a table on the results of a window functions. Say I wanted the highest paid employee in each department, I could so with the following nested query,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3dd51d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────────┬───────────────┬────────────┬────────┐\n",
       "│ employee_id │ employee_name │ department │ salary │\n",
       "│    int32    │    varchar    │  varchar   │ int32  │\n",
       "├─────────────┼───────────────┼────────────┼────────┤\n",
       "│           2 │ Bob Smith     │ Sales      │ 700000 │\n",
       "│          16 │ Samuel Turner │ Operations │  90000 │\n",
       "│          17 │ Tara Benson   │ Marketing  │  95000 │\n",
       "└─────────────┴───────────────┴────────────┴────────┘"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\"\n",
    "SELECT \n",
    "    employee_id,\n",
    "    employee_name,\n",
    "    department, \n",
    "    salary\n",
    "FROM (\n",
    "    SELECT \n",
    "        employee_id, \n",
    "        employee_name,\n",
    "        department,\n",
    "        salary,\n",
    "        RANK() OVER(PARTITION BY department ORDER BY salary DESC) AS dept_salary_rank\n",
    "    FROM\n",
    "        employees\n",
    "    ORDER BY employee_id\n",
    ") B\n",
    "WHERE \n",
    "    B.dept_salary_rank = 1\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf7ab0",
   "metadata": {},
   "source": [
    "Instead of having a nested query I can use the [QUALIFY](https://duckdb.org/docs/stable/sql/query_syntax/qualify) statement,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf1247a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────────┬───────────────┬────────────┬────────┐\n",
       "│ employee_id │ employee_name │ department │ salary │\n",
       "│    int32    │    varchar    │  varchar   │ int32  │\n",
       "├─────────────┼───────────────┼────────────┼────────┤\n",
       "│          16 │ Samuel Turner │ Operations │  90000 │\n",
       "│          17 │ Tara Benson   │ Marketing  │  95000 │\n",
       "│           2 │ Bob Smith     │ Sales      │ 700000 │\n",
       "└─────────────┴───────────────┴────────────┴────────┘"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\"\n",
    "    SELECT \n",
    "        employee_id, \n",
    "        employee_name,\n",
    "        department,\n",
    "        salary \n",
    "    FROM\n",
    "        employees\n",
    "    QUALIFY RANK() OVER(PARTITION BY department ORDER BY salary DESC) = 1\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7753408f",
   "metadata": {},
   "source": [
    "This is similary to the [HAVING clause](https://www.w3schools.com/sql/sql_having.asp) where a filtering condition is placed on the results of an aggregation. Both cases can be solved using a subquery and a WHERE clause. Nested queries can be easier to read, but I prefer explicitly listing columns instead of using a `*` (**PLEASE DONT DO THIS** it makes it impossible to tell where columns come from when you have long queries and mutliple joins) and having subqueries ontop of this makes very long queries which can be tricky to follow. \n",
    "\n",
    "\n",
    "With Spark Dataframes this is less of a big deal since its a simple filter condition,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccfbe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "win = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "# Apply a window function (e.g., row_number) and create a subquery/CTE equivalent\n",
    "df = employee_df.withColumn(\"rank\", row_number().over(win))\n",
    "\n",
    "# Filter the results based on the window function output\n",
    "df.where(\"rank <= 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d926f8af",
   "metadata": {},
   "source": [
    "However, one can also can always turn to [Common Table Expressions (CTEs)](https://www.geeksforgeeks.org/sql/cte-in-sql/) or [VIEWS](https://spark.apache.org/docs/latest/sql-ref-syntax-ddl-create-view.html) in SparkSQL, but if you pull out QUALIFY you get automatic street credit. :-) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4146e1f9",
   "metadata": {},
   "source": [
    "### 6. LEAD & LAG\n",
    "\n",
    "Now we can get started with window functions for time sersies data! Windw functions are great for calculating running totals, moving averages, and other time-based aggregations. A simple example is finding the value for a cell at the prior time period. For instance say we have the following sales data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "799ab27a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────┬─────────────┬────────────┬────────┐\n",
       "│ sale_id │  category   │ sale_date  │ amount │\n",
       "│  int32  │   varchar   │    date    │ int32  │\n",
       "├─────────┼─────────────┼────────────┼────────┤\n",
       "│       1 │ Electronics │ 2024-01-15 │   1200 │\n",
       "│       2 │ Furniture   │ 2024-01-20 │    800 │\n",
       "│       3 │ Electronics │ 2024-02-10 │   1500 │\n",
       "│       4 │ Clothing    │ 2024-02-15 │    300 │\n",
       "│       5 │ Furniture   │ 2024-03-05 │    700 │\n",
       "│       6 │ Clothing    │ 2024-03-10 │    400 │\n",
       "│       7 │ Electronics │ 2024-03-15 │   2000 │\n",
       "│       8 │ Furniture   │ 2024-03-22 │    950 │\n",
       "│       9 │ Electronics │ 2024-04-02 │   1750 │\n",
       "│      10 │ Clothing    │ 2024-04-08 │    280 │\n",
       "│      11 │ Electronics │ 2024-04-18 │   2200 │\n",
       "│      12 │ Furniture   │ 2024-04-25 │    640 │\n",
       "│      13 │ Clothing    │ 2024-05-01 │    520 │\n",
       "│      14 │ Electronics │ 2024-05-12 │   1950 │\n",
       "│      15 │ Furniture   │ 2024-05-20 │    890 │\n",
       "│      16 │ Clothing    │ 2024-06-03 │    310 │\n",
       "│      17 │ Electronics │ 2024-06-10 │   2400 │\n",
       "│      18 │ Furniture   │ 2024-06-15 │    760 │\n",
       "│      19 │ Clothing    │ 2024-06-21 │    450 │\n",
       "│      20 │ Electronics │ 2024-07-05 │   1800 │\n",
       "├─────────┴─────────────┴────────────┴────────┤\n",
       "│ 20 rows                           4 columns │\n",
       "└─────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\"\n",
    "SELECT\n",
    "    sale_id,\n",
    "    category,\n",
    "    sale_date,\n",
    "    amount\n",
    "FROM \n",
    "    sales\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57c7d68",
   "metadata": {},
   "source": [
    "We can find month-over-month change in sales using the [LAG](https://www.geeksforgeeks.org/sql/sql-server-lag-function-overview/). For clarity I'll create a monthly sales table first, notice how I have to extract the `YEAR` and `MONTH` and concatenate them into an integer as opposed to a string so there is an ordering for the window function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f7ce65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.query(\"\"\"\n",
    "DROP TABLE IF EXISTS monthly_sales;\n",
    "CREATE TABLE monthly_sales AS (\n",
    "SELECT\n",
    "    category,\n",
    "    CAST(\n",
    "        CONCAT(YEAR(sale_date)::VARCHAR, LPAD(MONTH(sale_date)::VARCHAR, 2, '0'))\n",
    "        AS INTEGER\n",
    "    ) AS year_month,\n",
    "    SUM(amount) AS amount\n",
    "FROM \n",
    "    sales\n",
    "GROUP BY 1, 2\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5941c5",
   "metadata": {},
   "source": [
    "Now I can find the prior month sales with the LAG function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b9005e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────────┬────────────┬───────────────────┬────────────────────┬─────────────────────────┐\n",
       "│  category   │ year_month │ this_month_amount │ prior_month_amount │ month_over_month_change │\n",
       "│   varchar   │   int32    │      int128       │       int128       │         double          │\n",
       "├─────────────┼────────────┼───────────────────┼────────────────────┼─────────────────────────┤\n",
       "│ Clothing    │     202402 │               300 │               NULL │                    NULL │\n",
       "│ Clothing    │     202403 │               400 │                300 │      33.333333333333336 │\n",
       "│ Clothing    │     202404 │               280 │                400 │                   -30.0 │\n",
       "│ Clothing    │     202405 │               520 │                280 │       85.71428571428571 │\n",
       "│ Clothing    │     202406 │               760 │                520 │       46.15384615384615 │\n",
       "│ Electronics │     202401 │              1200 │               NULL │                    NULL │\n",
       "│ Electronics │     202402 │              1500 │               1200 │                    25.0 │\n",
       "│ Electronics │     202403 │              2000 │               1500 │      33.333333333333336 │\n",
       "│ Electronics │     202404 │              3950 │               2000 │                    97.5 │\n",
       "│ Electronics │     202405 │              1950 │               3950 │      -50.63291139240506 │\n",
       "│ Electronics │     202406 │              2400 │               1950 │      23.076923076923077 │\n",
       "│ Electronics │     202407 │              1800 │               2400 │                   -25.0 │\n",
       "│ Furniture   │     202401 │               800 │               NULL │                    NULL │\n",
       "│ Furniture   │     202403 │              1650 │                800 │                  106.25 │\n",
       "│ Furniture   │     202404 │               640 │               1650 │      -61.21212121212121 │\n",
       "│ Furniture   │     202405 │               890 │                640 │                 39.0625 │\n",
       "│ Furniture   │     202406 │               760 │                890 │     -14.606741573033707 │\n",
       "├─────────────┴────────────┴───────────────────┴────────────────────┴─────────────────────────┤\n",
       "│ 17 rows                                                                           5 columns │\n",
       "└─────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\"\n",
    "SELECT \n",
    "    category,\n",
    "    year_month,\n",
    "    this_month_amount,\n",
    "    prior_month_amount,\n",
    "    100 * (this_month_amount - prior_month_amount) / prior_month_amount AS month_over_month_change\n",
    "FROM (\n",
    "    SELECT\n",
    "        category,\n",
    "        year_month,\n",
    "        amount AS this_month_amount,\n",
    "        LAG(amount) OVER(PARTITION BY category ORDER BY year_month ASC) AS prior_month_amount,\n",
    "    FROM \n",
    "        monthly_sales\n",
    "    ORDER BY 1,2 ASC\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c9e944",
   "metadata": {},
   "source": [
    "### 7.  Moving Averages\n",
    "\n",
    "The last window function technique I'll mention related to time series is taking the rolling average or moving average of a value over a window. The window in this case is determined both by the `PARTITION BY` statement as well as a `ROWS BETWEEN ...` statement. For isntance, if I wanted to get the rolling 3 month average of sales per category I could say, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51251ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────────┬────────────┬────────┬────────────────────────────┐\n",
       "│  category   │ year_month │ amount │ three_month_moving_average │\n",
       "│   varchar   │   int32    │ int128 │           double           │\n",
       "├─────────────┼────────────┼────────┼────────────────────────────┤\n",
       "│ Electronics │     202401 │   1200 │                     1200.0 │\n",
       "│ Electronics │     202402 │   1500 │                     1350.0 │\n",
       "│ Electronics │     202403 │   2000 │         1566.6666666666667 │\n",
       "│ Electronics │     202404 │   3950 │         2483.3333333333335 │\n",
       "│ Electronics │     202405 │   1950 │         2633.3333333333335 │\n",
       "│ Electronics │     202406 │   2400 │         2766.6666666666665 │\n",
       "│ Electronics │     202407 │   1800 │                     2050.0 │\n",
       "│ Clothing    │     202402 │    300 │                      300.0 │\n",
       "│ Clothing    │     202403 │    400 │                      350.0 │\n",
       "│ Clothing    │     202404 │    280 │          326.6666666666667 │\n",
       "│ Clothing    │     202405 │    520 │                      400.0 │\n",
       "│ Clothing    │     202406 │    760 │                      520.0 │\n",
       "│ Furniture   │     202401 │    800 │                      800.0 │\n",
       "│ Furniture   │     202403 │   1650 │                     1225.0 │\n",
       "│ Furniture   │     202404 │    640 │                     1030.0 │\n",
       "│ Furniture   │     202405 │    890 │                     1060.0 │\n",
       "│ Furniture   │     202406 │    760 │          763.3333333333334 │\n",
       "├─────────────┴────────────┴────────┴────────────────────────────┤\n",
       "│ 17 rows                                              4 columns │\n",
       "└────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\"\n",
    "SELECT\n",
    "    category,\n",
    "    year_month,\n",
    "    amount,\n",
    "    AVG(amount) OVER(PARTITION BY category \n",
    "                    ORDER BY year_month ASC \n",
    "                    ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS three_month_moving_average\n",
    "FROM\n",
    "    monthly_sales\n",
    "\n",
    "\"\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3c2f35",
   "metadata": {},
   "source": [
    "Notice with the 3 month moving average, the first month is the current month amount and second month is the average of the second month and the prior month amount. This is because there are not enough prior months to calculate 3 month the average. Depending on the dialect of SQL you are using the first two months are for each category will be NULL instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecfafd6",
   "metadata": {},
   "source": [
    "### 8. Performance Issues With Multiple Expressions in Apache Spark\n",
    "\n",
    "One thing I'll mention in Spark is that when using dataframes you could be looking to do a lot of different variations of queries. For instance, we could be looking at multiple different moving averages. One way we could do this is to create a loop over each moving average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98d2238",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(duckdb.query(\"SELECT category, year_month, amount FROM monthly_sales\").to_df())\n",
    "\n",
    "for i in range(3,6):\n",
    "    df = df.withColumn(\n",
    "        f\"{i}_month_moving_average\",\n",
    "        F.avg(\"amount\").over(\n",
    "            Window.partitionBy(\"category\")\n",
    "            .orderBy(\"year_month\")\n",
    "            .rowsBetween(i - 1, 0)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81153fd5",
   "metadata": {},
   "source": [
    "This isnt so bad when you have a few iterations in the loop, but if you have many `withColumns` this can lead to issues with [your query plan exploding due to projecions and cause performance issues](https://community.databricks.com/t5/technical-blog/performance-showdown-withcolumn-vs-withcolumns-in-apache-spark/ba-p/129142). The query plan can see using the [.explain](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.explain.html) method,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947aff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453ca858",
   "metadata": {},
   "source": [
    "With Spark 3.0 they introduced a [withColumns](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumns.html) which allows you to efficiently apply multiplel queries at once using a dictionary as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2e28c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_dict = {\n",
    "    f\"{i}_month_moving_average\": \n",
    "            F.avg(\"amount\").over(\n",
    "                Window.partitionBy(\"category\")\n",
    "                .orderBy(\"year_month\")\n",
    "                .rowsBetween(i - 1, 0)) \n",
    "    for i in range(3,6)\n",
    "}\n",
    "\n",
    "df = df.withColumns(queries_dict)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2846035",
   "metadata": {},
   "source": [
    "## IV. Array Operations\n",
    "-------------------\n",
    "\n",
    "In my opinon array operations are a pretty niche topic in SQL, however, when you need them they are a lifesaver!! One common way to use arrays is to collect information about an enty. \n",
    "\n",
    "### 9. COLLECT_SET/ARRAY_AGG\n",
    "\n",
    "Say for instance we run a real-estate sales business with the following sales data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d945a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────────────┬────────────┬────────────┬─────────────┐\n",
       "│ employee_name │ sale_date  │ home_price │    city     │\n",
       "│    varchar    │    date    │   int32    │   varchar   │\n",
       "├───────────────┼────────────┼────────────┼─────────────┤\n",
       "│ Alice Johnson │ 2024-01-10 │     350000 │ New York    │\n",
       "│ Bob Smith     │ 2024-01-15 │     450000 │ Los Angeles │\n",
       "│ Charlie Brown │ 2024-02-05 │     300000 │ Chicago     │\n",
       "│ Diana Prince  │ 2024-02-20 │     600000 │ Miami       │\n",
       "│ Ethan Hunt    │ 2024-03-12 │     400000 │ Seattle     │\n",
       "│ Alice Johnson │ 2024-03-25 │     550000 │ New York    │\n",
       "│ Bob Smith     │ 2024-04-10 │     700000 │ Los Angeles │\n",
       "│ Charlie Brown │ 2024-04-18 │     320000 │ Chicago     │\n",
       "│ Diana Prince  │ 2024-05-05 │     620000 │ Miami       │\n",
       "│ Ethan Hunt    │ 2024-05-22 │     410000 │ Seattle     │\n",
       "│ Frank Green   │ 2024-06-01 │     480000 │ Dallas      │\n",
       "│ Grace Adams   │ 2024-06-15 │     510000 │ Phoenix     │\n",
       "│ Henry King    │ 2024-07-07 │     390000 │ Denver      │\n",
       "│ Ivy Walker    │ 2024-07-21 │     530000 │ Atlanta     │\n",
       "│ Jack White    │ 2024-08-09 │     460000 │ Boston      │\n",
       "│ Frank Green   │ 2024-08-28 │     490000 │ New York    │\n",
       "│ Grace Adams   │ 2024-09-14 │     520000 │ Los Angeles │\n",
       "│ Henry King    │ 2024-10-03 │     400000 │ Denver      │\n",
       "│ Ivy Walker    │ 2024-10-17 │     540000 │ Atlanta     │\n",
       "│ Jack White    │ 2024-11-05 │     470000 │ Miami       │\n",
       "├───────────────┴────────────┴────────────┴─────────────┤\n",
       "│ 20 rows                                     4 columns │\n",
       "└───────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\"\n",
    "SELECT\n",
    "    employee_name,\n",
    "    sale_date,\n",
    "    home_price,\n",
    "    city\n",
    "FROM\n",
    "    home_sales\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0058ff65",
   "metadata": {},
   "source": [
    "Say we wanted to know which employees sell homes in more than one city and what those cities are. To get the employees that sell in the more that one city this is a simple [GROUP BY](https://www.w3schools.com/sql/sql_groupby.asp) and `COUNT`, but to get the cities is a little tricker. A simple way is to collect each city an employee sells in an array adn then only take the arrays with more than one entry. The [ARRAY_AGG](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.array_agg.html) function allows us to collect the entries in group by statement as an array as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c333699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.query(\"\"\"\n",
    "DROP TABLE IF EXISTS employees_with_multiple_cities;\n",
    "CREATE TABLE employees_with_multiple_cities AS \n",
    "SELECT \n",
    "    employee_name,\n",
    "    ARRAY_AGG(city) AS cities_sold_in\n",
    "FROM (\n",
    "    SELECT\n",
    "        employee_name,\n",
    "        city\n",
    "    FROM\n",
    "        home_sales\n",
    "    GROUP BY 1,2\n",
    ") BASE\n",
    "GROUP BY 1\n",
    "HAVING LENGTH(cities_sold_in) > 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9138506e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────────────┬────────────────────────┐\n",
       "│ employee_name │     cities_sold_in     │\n",
       "│    varchar    │       varchar[]        │\n",
       "├───────────────┼────────────────────────┤\n",
       "│ Frank Green   │ [New York, Dallas]     │\n",
       "│ Grace Adams   │ [Phoenix, Los Angeles] │\n",
       "│ Jack White    │ [Boston, Miami]        │\n",
       "└───────────────┴────────────────────────┘"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"SELECT employee_name, cities_sold_in FROM employees_with_multiple_cities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a60ac",
   "metadata": {},
   "source": [
    "Notice I had first get the unique set of employee and cities they sold in removing dupications in cities. With Spark this is even easier because they have a [COLLECT_SET](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.collect_set.html) which removes dupications so I can write the query as,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2950e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = spark.createDataFrame(duckdb.query(\"SELECT employee_name, city FROM home_sales\").to_df())\n",
    "\n",
    "result_df = (sales_df.groupBy(\"employee_name\")\n",
    "                     .agg(F.collect_set(\"city\").alias(\"cities_sold_in\")) \n",
    "                     .where(F.size(\"cities_sold_in\") > 1)\n",
    ") \n",
    "\n",
    "result_df.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6164c7b8",
   "metadata": {},
   "source": [
    "Now what if I didnt want to have the cities as a list, but instead listed out as a row?\n",
    "### 10. EXPLODE/UNEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cb7466",
   "metadata": {},
   "source": [
    "The [explode](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.collect_set.html) does just this, it is the opposite of collecting elements in an array! In DuckDB this is called [UNNEST](https://duckdb.org/docs/stable/sql/query_syntax/unnest). As a simple example see how the `UNNEST` function works,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "98aea57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────┐\n",
       "│ letter  │\n",
       "│ varchar │\n",
       "├─────────┤\n",
       "│ A       │\n",
       "│ B       │\n",
       "│ C       │\n",
       "└─────────┘"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"SELECT UNNEST(ARRAY['A', 'B', 'C']) AS letter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f92cf47",
   "metadata": {},
   "source": [
    "Now say we want to unnest the cities in the above results, we can do this with the `UNNEST` function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b4f238ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────────────┬─────────────┐\n",
       "│ employee_name │    city     │\n",
       "│    varchar    │   varchar   │\n",
       "├───────────────┼─────────────┤\n",
       "│ Frank Green   │ New York    │\n",
       "│ Frank Green   │ Dallas      │\n",
       "│ Grace Adams   │ Phoenix     │\n",
       "│ Grace Adams   │ Los Angeles │\n",
       "│ Jack White    │ Boston      │\n",
       "│ Jack White    │ Miami       │\n",
       "└───────────────┴─────────────┘"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\"\n",
    "SELECT\n",
    "    employee_name,\n",
    "    UNNEST(cities_sold_in) AS city\n",
    "FROM\n",
    "    employees_with_multiple_cities \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b6a3c",
   "metadata": {},
   "source": [
    "Notice how the rows that were not exploded are repeated for each value in the array (i.e. twice for each employee). In Spark, this is simple as well,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0addfdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = (result_df.withColumn(\"city\", F.explode(\"cities_sold_in\"))\n",
    "                      .select(\"employee_name\", \"city\"))\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0d92cd",
   "metadata": {},
   "source": [
    "This is a simple example, but once you start considering using arrays in your SQL queries the types of things you can do in a few lines grows tremendously!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feb6253",
   "metadata": {},
   "source": [
    "## IV. Special Joins\n",
    "--------------------\n",
    "Now to the topic of special joins which will be a little bit different than the prior sections. In the prior sections I covered how to do things with techniques and in this section, I'll instead focus how do things more effectively using joins.\n",
    "\n",
    "### 11. Using A JOIN Instead Of CASE WHEN\n",
    "The first thing is using a `JOIN` instead of a `CASE WHEN` statement. For instance suppose we wanted to append a new column to the `home_sales` table called `region` where we group the following,\n",
    "\n",
    "| Region    | Cities            |\n",
    "|:----------:|:----------------:|\n",
    "| Northeast | Boston, New York  |\n",
    "| South     | Miami, Atlanta    |\n",
    "| Pacific   | Seattle, Los Angeles |\n",
    "| Southwest | Pheonix, Denver, Dallas |\n",
    "| Midwest   | Chicago |\n",
    "\n",
    "This can be added as a `CASE WHEN` statement,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8ee0522a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────────────┬────────────┬────────────┬─────────────┬───────────┐\n",
       "│ employee_name │ sale_date  │ home_price │    city     │  region   │\n",
       "│    varchar    │    date    │   int32    │   varchar   │  varchar  │\n",
       "├───────────────┼────────────┼────────────┼─────────────┼───────────┤\n",
       "│ Alice Johnson │ 2024-01-10 │     350000 │ New York    │ Northeast │\n",
       "│ Bob Smith     │ 2024-01-15 │     450000 │ Los Angeles │ Pacific   │\n",
       "│ Charlie Brown │ 2024-02-05 │     300000 │ Chicago     │ Midwest   │\n",
       "│ Diana Prince  │ 2024-02-20 │     600000 │ Miami       │ South     │\n",
       "│ Ethan Hunt    │ 2024-03-12 │     400000 │ Seattle     │ Pacific   │\n",
       "│ Alice Johnson │ 2024-03-25 │     550000 │ New York    │ Northeast │\n",
       "│ Bob Smith     │ 2024-04-10 │     700000 │ Los Angeles │ Pacific   │\n",
       "│ Charlie Brown │ 2024-04-18 │     320000 │ Chicago     │ Midwest   │\n",
       "│ Diana Prince  │ 2024-05-05 │     620000 │ Miami       │ South     │\n",
       "│ Ethan Hunt    │ 2024-05-22 │     410000 │ Seattle     │ Pacific   │\n",
       "│ Frank Green   │ 2024-06-01 │     480000 │ Dallas      │ Southwest │\n",
       "│ Grace Adams   │ 2024-06-15 │     510000 │ Phoenix     │ Southwest │\n",
       "│ Henry King    │ 2024-07-07 │     390000 │ Denver      │ Southwest │\n",
       "│ Ivy Walker    │ 2024-07-21 │     530000 │ Atlanta     │ South     │\n",
       "│ Jack White    │ 2024-08-09 │     460000 │ Boston      │ Northeast │\n",
       "│ Frank Green   │ 2024-08-28 │     490000 │ New York    │ Northeast │\n",
       "│ Grace Adams   │ 2024-09-14 │     520000 │ Los Angeles │ Pacific   │\n",
       "│ Henry King    │ 2024-10-03 │     400000 │ Denver      │ Southwest │\n",
       "│ Ivy Walker    │ 2024-10-17 │     540000 │ Atlanta     │ South     │\n",
       "│ Jack White    │ 2024-11-05 │     470000 │ Miami       │ South     │\n",
       "├───────────────┴────────────┴────────────┴─────────────┴───────────┤\n",
       "│ 20 rows                                                 5 columns │\n",
       "└───────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\"\n",
    "SELECT\n",
    "    employee_name,\n",
    "    sale_date,\n",
    "    home_price,\n",
    "    city,\n",
    "    CASE\n",
    "        WHEN city IN ('Boston', 'New York') THEN 'Northeast'\n",
    "        WHEN city IN ('Miami', 'Atlanta') THEN 'South'\n",
    "        WHEN city IN ('Seattle', 'Los Angeles') THEN 'Pacific'\n",
    "        WHEN city IN ('Phoenix', 'Denver', 'Dallas') THEN 'Southwest'\n",
    "        WHEN city IN ('Chicago') THEN 'Midwest'\n",
    "    END AS region\n",
    "FROM\n",
    "    home_sales\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c76852",
   "metadata": {},
   "source": [
    "\n",
    "But what happens if we add a new city or region? We need to go back and change our query or add a default case. Instead we could create a mapping table and use a join to assign the region. This can be achieved using the mapping table below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "62721f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────────┬─────────────┐\n",
       "│  region   │    city     │\n",
       "│  varchar  │   varchar   │\n",
       "├───────────┼─────────────┤\n",
       "│ Northeast │ New York    │\n",
       "│ Pacific   │ Los Angeles │\n",
       "│ Midwest   │ Chicago     │\n",
       "│ South     │ Miami       │\n",
       "│ Pacific   │ Seattle     │\n",
       "│ Southwest │ Dallas      │\n",
       "│ Southwest │ Phoenix     │\n",
       "│ Southwest │ Denver      │\n",
       "│ South     │ Atlanta     │\n",
       "│ Northeast │ Boston      │\n",
       "├───────────┴─────────────┤\n",
       "│ 10 rows       2 columns │\n",
       "└─────────────────────────┘"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\"\n",
    "SELECT\n",
    "    region,\n",
    "    city\n",
    "FROM\n",
    "    regions\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dc905a",
   "metadata": {},
   "source": [
    "And joining it to the `home_sales` table,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f43bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────────────┬────────────┬────────────┬─────────────┬───────────┐\n",
       "│ employee_name │ sale_date  │ home_price │    city     │  region   │\n",
       "│    varchar    │    date    │   int32    │   varchar   │  varchar  │\n",
       "├───────────────┼────────────┼────────────┼─────────────┼───────────┤\n",
       "│ Alice Johnson │ 2024-01-10 │     350000 │ New York    │ Northeast │\n",
       "│ Bob Smith     │ 2024-01-15 │     450000 │ Los Angeles │ Pacific   │\n",
       "│ Charlie Brown │ 2024-02-05 │     300000 │ Chicago     │ Midwest   │\n",
       "│ Diana Prince  │ 2024-02-20 │     600000 │ Miami       │ South     │\n",
       "│ Ethan Hunt    │ 2024-03-12 │     400000 │ Seattle     │ Pacific   │\n",
       "│ Alice Johnson │ 2024-03-25 │     550000 │ New York    │ Northeast │\n",
       "│ Bob Smith     │ 2024-04-10 │     700000 │ Los Angeles │ Pacific   │\n",
       "│ Charlie Brown │ 2024-04-18 │     320000 │ Chicago     │ Midwest   │\n",
       "│ Diana Prince  │ 2024-05-05 │     620000 │ Miami       │ South     │\n",
       "│ Ethan Hunt    │ 2024-05-22 │     410000 │ Seattle     │ Pacific   │\n",
       "│ Frank Green   │ 2024-06-01 │     480000 │ Dallas      │ Southwest │\n",
       "│ Grace Adams   │ 2024-06-15 │     510000 │ Phoenix     │ Southwest │\n",
       "│ Henry King    │ 2024-07-07 │     390000 │ Denver      │ Southwest │\n",
       "│ Ivy Walker    │ 2024-07-21 │     530000 │ Atlanta     │ South     │\n",
       "│ Jack White    │ 2024-08-09 │     460000 │ Boston      │ Northeast │\n",
       "│ Frank Green   │ 2024-08-28 │     490000 │ New York    │ Northeast │\n",
       "│ Grace Adams   │ 2024-09-14 │     520000 │ Los Angeles │ Pacific   │\n",
       "│ Henry King    │ 2024-10-03 │     400000 │ Denver      │ Southwest │\n",
       "│ Ivy Walker    │ 2024-10-17 │     540000 │ Atlanta     │ South     │\n",
       "│ Jack White    │ 2024-11-05 │     470000 │ Miami       │ South     │\n",
       "├───────────────┴────────────┴────────────┴─────────────┴───────────┤\n",
       "│ 20 rows                                                 5 columns │\n",
       "└───────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\"\n",
    "SELECT \n",
    "    L.employee_name,\n",
    "    L.sale_date,\n",
    "    L.home_price,\n",
    "    L.city,\n",
    "    R.region\n",
    "FROM\n",
    "    home_sales AS L\n",
    "LEFT JOIN \n",
    "    regions AS R\n",
    "ON \n",
    "    L.city = R.city\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c8de27",
   "metadata": {},
   "source": [
    "Often changing the region mapping table is easier and more natural than changing the case when staments in production.\n",
    "\n",
    "This case leads us to another issue specific to Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3444586d",
   "metadata": {},
   "source": [
    "### 12. Broadcast Join in SparkSQL\n",
    "The [broadcast join](https://sparkbyexamples.com/pyspark/pyspark-broadcast-join-with-example/) is an extremely helpful join when joining datasests where one dataset it very large and the other is small. Imagine if we had hundreds of millions or billions of sales (for homes this is unlikely), but the region mapping was only 50 rows. Since the large table is distributed over many executors and the smaller table is not this can lead to poor performance since there will be much shuffling to make sure the join keys exist on all the executors perfoming the join. Instead, one can broadcast the smaller table to all the executors and then perform the join. This is depicted below (they use \"worker\" instead of \"executor\", but they're the same),\n",
    "\n",
    "<p align=\"center\">\n",
    "    <figure>\n",
    "        <img src=\"https://miro.medium.com/v2/resize:fit:1360/format:webp/1*Lsb6aNS0n8IT4o2MBcTDqA.png?raw=1\" width=\"500\" class=\"center\">\n",
    "    <figcaption>\n",
    "    Source: https://medium.com/@amarkrgupta96/join-strategies-in-apache-spark-a-hands-on-approach-d0696fc0a6c9\n",
    "    </figcaption>\n",
    "    </figure>\n",
    "</p>\n",
    "\n",
    "One can do this in code by the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033dd54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_df = spark.createDataFrame(duckdb.query(\"SELECT region, city FROM regions\").to_df())\n",
    "\n",
    "sales_df.join(F.broadcast(regions_df), [\"city\"], \"left\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aef149",
   "metadata": {},
   "source": [
    "Note there is a limit to how big a table you can broadcast. You also no longer have to explicitly broadcast the smaller table, spark can infer this based on your settings! Check out the [SQL Performance Tuning](https://spark.apache.org/docs/latest/sql-performance-tuning.html#automatically-broadcasting-joins) page on the Spark documentation site.\n",
    "\n",
    "### 13. Filtering by Join\n",
    "This leads us to another good use of joins: using inner joins to filter our data. Suppose we want to find only those sales that occured in the northeast. We could first join the two tables and then filter `region = 'Northeast'`, but this is wasteful since we'll do a huge join only to remove many of the rows. Instead we can use the inner join to \"filter\" out the results we dont want. In the example with home sales, we would first create an inner query to select only cities in the Northeast and then join that to the results table. The query correspond to this is,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "34962cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────────────┬────────────┬────────────┬──────────┐\n",
       "│ employee_name │ sale_date  │ home_price │   city   │\n",
       "│    varchar    │    date    │   int32    │ varchar  │\n",
       "├───────────────┼────────────┼────────────┼──────────┤\n",
       "│ Alice Johnson │ 2024-01-10 │     350000 │ New York │\n",
       "│ Alice Johnson │ 2024-03-25 │     550000 │ New York │\n",
       "│ Jack White    │ 2024-08-09 │     460000 │ Boston   │\n",
       "│ Frank Green   │ 2024-08-28 │     490000 │ New York │\n",
       "└───────────────┴────────────┴────────────┴──────────┘"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\"\n",
    "SELECT \n",
    "    L.employee_name,\n",
    "    L.sale_date,\n",
    "    L.home_price,\n",
    "    L.city,\n",
    "FROM\n",
    "    home_sales AS L\n",
    "JOIN \n",
    "    (SELECT \n",
    "        city \n",
    "    FROM \n",
    "        regions \n",
    "    WHERE  \n",
    "        region = 'Northeast'\n",
    "    ) AS R\n",
    "ON \n",
    "    L.city = R.city\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd37ef2e",
   "metadata": {},
   "source": [
    "Many SQL languages the order of when you apply the `WHERE` clause, i.e. before or after join can be decided by the query optimizer. Spark uses a lazy evaluation model which means that the query optimizer can make these decisions for you. For instance, the query plan for having the where condition before the join,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad4d2bd",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "(sales_df.join(F.broadcast(regions_df.where(\"region = 'Northeast'\")), [\"city\"], \"inner\" )\n",
    "         .explain())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f745bd1e",
   "metadata": {},
   "source": [
    "will produce the same query plan as having the `WHERE` condition after the join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b6b49c",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "(sales_df.join(F.broadcast(regions_df), [\"city\"], \"inner\").where(\"region = 'Northeast'\")\n",
    "         .explain())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4efae3",
   "metadata": {},
   "source": [
    "One thing to be careful of with Spark when using a join for filter you are often using a smaller dataframe for filtering. A broadcast join can be appropriate when the other table is very large, but there is another consideration to make. The overlap in the join keys (i.e. city values) between the small dataframe which was broadcasted and the large dataframe only occurs on a few executors in the distributed dataframe table. This leads to many executors having 0 or very few resulting rows following the join. This can lead to data skew in the resulting dataframe. This reveals itself in downstream operations on the dataframe where many executors complete their work quickly and a few take an extremely long time. You can see this in the Spark UI as shown below\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <figure>\n",
    "        <img src=\"https://www.unraveldata.com/wp-content/uploads/2019/04/before-1.jpg?raw=1\" width=\"700\" class=\"center\">\n",
    "    <figcaption>\n",
    "    Source: https://www.unraveldata.com/common-failures-slowdowns-part-ii/\n",
    "    </figcaption>\n",
    "    </figure>\n",
    "</p>\n",
    "\n",
    "The data skew issue can be corrected by using the [coalesce](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.coalesce.html) or [repartition](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.repartition.html) to reduce number of partitions and more evenly distribute the data across the executors. An example is below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8631e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = (sales_df.join(F.broadcast(regions_df), [\"city\"], \"inner\").where(\"region = 'Northeast'\")\n",
    "                      .coalesce(10))\n",
    "                      \n",
    "results_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb631f6a",
   "metadata": {},
   "source": [
    "### 14. Anti Joins\n",
    "Now to the last topic in SQL. The left anti join in Spark is another technique that is not as well known and will garner street credit.\n",
    "\n",
    "Say we to see which real-estate agents have no sales. We can use an [ANTI JOIN](https://duckdb.org/docs/stable/sql/query_syntax/from#semi-and-anti-joins) of the employees on the sales data to do so. An `ANTI JOIN` will return all records in the left table that have no match in the right table. The syntax in DuckDB is,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "301f303c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────────────┬────────┐\n",
       "│ employee_name │ salary │\n",
       "│    varchar    │ int32  │\n",
       "├───────────────┼────────┤\n",
       "│ Marcus Lee    │ 420000 │\n",
       "│ Nina Patel    │ 450000 │\n",
       "│ Laura Chen    │ 380000 │\n",
       "└───────────────┴────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\"\n",
    "SELECT \n",
    "    L.employee_name,\n",
    "    L.salary\n",
    "FROM \n",
    "    employees L \n",
    "ANTI JOIN\n",
    "    home_sales AS R\n",
    "ON \n",
    "    L.employee_name = R.employee_name\n",
    "WHERE\n",
    "    department = 'Sales'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b369fd31",
   "metadata": {},
   "source": [
    "To do this query without an ANTI JOIN we would have to filter on all employees with NULL sales,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f43d7184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────────────┬────────┐\n",
       "│ employee_name │ salary │\n",
       "│    varchar    │ int32  │\n",
       "├───────────────┼────────┤\n",
       "│ Marcus Lee    │ 420000 │\n",
       "│ Nina Patel    │ 450000 │\n",
       "│ Laura Chen    │ 380000 │\n",
       "└───────────────┴────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\"\"\"\n",
    "SELECT \n",
    "    L.employee_name,\n",
    "    L.salary\n",
    "FROM \n",
    "    employees L \n",
    "LEFT JOIN\n",
    "    home_sales AS R\n",
    "ON \n",
    "    L.employee_name = R.employee_name\n",
    "WHERE\n",
    "    department = 'Sales'\n",
    "AND \n",
    "    R.home_price IS NULL\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180604a",
   "metadata": {},
   "source": [
    "Depending on the problem, you sometimes need to apply the NULL filter in an outer query or else you will inadvertently remove records. In SparkSQL the syntax is the same and the dataframe syntax is,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9864a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.join(sales_df, [\"employee_name\"] \"left-anti\").where(\"department = 'Sales'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61332909",
   "metadata": {},
   "source": [
    "## V. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fbab43",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
